{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rzarostmi/project_lightfm_suggest_Sahanuni/blob/main/Copy_of_Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Welcome to Colab!\n",
        "\n",
        "\n",
        "## Access Popular LLMs via Google-Colab-AI Without an API Key\n",
        "Users with Colab's paid plans have free access to most popular LLMs via google-colab-ai Python library. For more details, refer to the [getting started with google colab ai](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb).\n",
        "\n",
        "```\n",
        "from google.colab import ai\n",
        "response = ai.generate_text(\"What is the capital of France?\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "\n",
        "## Explore the Gemini API\n",
        "The Gemini API gives you access to Gemini models created by Google DeepMind. Gemini models are built from the ground up to be multimodal, so you can reason seamlessly across text, images, code, and audio.\n",
        "\n",
        "**How to get started?**\n",
        "*  Go to [Google AI Studio](https://aistudio.google.com/) and log in with your Google account.\n",
        "*  [Create an API key](https://aistudio.google.com/app/apikey).\n",
        "* Use a quickstart for [Python](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb), or call the REST API using [curl](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Prompting_REST.ipynb).\n",
        "\n",
        "**Discover Gemini's advanced capabilities**\n",
        "*  Play with Gemini [multimodal outputs](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Image-out.ipynb), mixing text and images in an iterative way.\n",
        "*  Discover the [multimodal Live API](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb ) (demo [here](https://aistudio.google.com/live)).\n",
        "*  Learn how to [analyze images and detect items in your pictures](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\") using Gemini (bonus, there's a [3D version](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb) as well!).\n",
        "*  Unlock the power of [Gemini thinking model](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb), capable of solving complex task with its inner thoughts.\n",
        "      \n",
        "**Explore complex use cases**\n",
        "*  Use [Gemini grounding capabilities](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Search_grounding_for_research_report.ipynb) to create a report on a company based on what the model can find on internet.\n",
        "*  Extract [invoices and form data from PDF](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb) in a structured way.\n",
        "*  Create [illustrations based on a whole book](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Book_illustration.ipynb) using Gemini large context window and Imagen.\n",
        "\n",
        "To learn more, check out the [Gemini cookbook](https://github.com/google-gemini/cookbook) or visit the [Gemini API documentation](https://ai.google.dev/docs/).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import sys\n",
        "from scipy.sparse import coo_matrix\n",
        "from lightfm import LightFM\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "\n",
        "# ---------------------\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ ÙØ§Ø±Ø³ÛŒ\n",
        "# ---------------------\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler('lightfm_fast_scenarios.log')\n",
        "    ch = logging.StreamHandler(sys.stdout)\n",
        "    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(fmt)\n",
        "    ch.setFormatter(fmt)\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "# ---------------------\n",
        "# 1) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡\n",
        "# ---------------------\n",
        "df = pd.read_csv('/content/order-Product_prior.csv')\n",
        "\n",
        "# ğŸ”¹ Ù†Ø³Ø®Ù‡ Ø³Ø±ÛŒØ¹ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª)\n",
        "df = df.head(50000)  # ÙÙ‚Ø· ÛµÛ°Ù‡Ø²Ø§Ø± Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„\n",
        "\n",
        "# ğŸ”¹ Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ùˆ Ú©Ù†Ø¯ (Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡)\n",
        "# df = pd.read_csv('order-Product_prior.csv')  # Ú©Ù„ Ø¯Ø§Ø¯Ù‡\n",
        "# (ØªÙˆØ¬Ù‡: ÙØ¹Ø§Ù„â€ŒÚ©Ø±Ø¯Ù† Ø§ÛŒÙ† Ø®Ø· Ùˆ Ø­Ø°Ù head(...) Ø¨Ø§Ø¹Ø« Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ùˆ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯)\n",
        "\n",
        "df.columns = ['user_id', 'product_id', 'add_to_cart_order', 'reordered']\n",
        "df['rating'] = df['reordered'].fillna(0).clip(0, 1).astype(float)\n",
        "\n",
        "# Ù†Ú¯Ø§Ø´Øª IDÙ‡Ø§ Ø¨Ù‡ Ø§Ù†Ø¯ÛŒØ³\n",
        "user_ids = df['user_id'].unique()\n",
        "item_ids = df['product_id'].unique()\n",
        "user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
        "item_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
        "\n",
        "df['u_idx'] = df['user_id'].map(user_map)\n",
        "df['i_idx'] = df['product_id'].map(item_map)\n",
        "\n",
        "interactions = coo_matrix((df['rating'], (df['u_idx'], df['i_idx'])),\n",
        "                          shape=(len(user_ids), len(item_ids)))\n",
        "logger.info(f\"âœ… Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³: {interactions.shape}\")\n",
        "\n",
        "# ---------------------\n",
        "# 2) ØªÙ‚Ø³ÛŒÙ… Ø¢Ù…ÙˆØ²Ø´/ØªØ³Øª\n",
        "# ---------------------\n",
        "train, test = random_train_test_split(interactions, test_percentage=0.2,\n",
        "                                      random_state=np.random.RandomState(42))\n",
        "\n",
        "# ---------------------\n",
        "# 3) Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
        "# ---------------------\n",
        "# ğŸ”¹ Ù†Ø³Ø®Ù‡ Ø³Ø±ÛŒØ¹\n",
        "model = LightFM(no_components=8, learning_rate=0.05, loss='warp')\n",
        "model.fit(train, epochs=5, num_threads=4)\n",
        "\n",
        "# ğŸ”¹ Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ùˆ Ú©Ù†Ø¯ (Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡)\n",
        "# model = LightFM(no_components=32, learning_rate=0.05, loss='warp')\n",
        "# model.fit(train, epochs=20, num_threads=4)\n",
        "\n",
        "logger.info(\"âœ… Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "# ---------------------\n",
        "# 4) ØªØ§Ø¨Ø¹ ØªÙˆØµÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ø±Ø¨Ø±\n",
        "# ---------------------\n",
        "def recommend_for_user(model, train_mat, user_idx, n=5):\n",
        "    n_users, n_items = train_mat.shape\n",
        "    scores = model.predict(\n",
        "        np.repeat(user_idx, n_items),\n",
        "        np.arange(n_items),\n",
        "        num_threads=4\n",
        "    )\n",
        "    known_items = set(train_mat.tocsr()[user_idx].indices)\n",
        "    candidates = [i for i in range(n_items) if i not in known_items]\n",
        "    top_idx = np.argsort(-scores[candidates])[:n]\n",
        "    return [candidates[i] for i in top_idx], scores\n",
        "\n",
        "# ---------------------\n",
        "# 5) Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ø±Ø¨Ø± Ù†Ù…ÙˆÙ†Ù‡)\n",
        "# ---------------------\n",
        "scenario_results = []\n",
        "target_user = df['u_idx'].iloc[0]\n",
        "user_all_items = set(train.tocsr()[target_user].indices) | set(test.tocsr()[target_user].indices)\n",
        "\n",
        "for remove_n in [1, 2, 3]:\n",
        "    if len(user_all_items) <= remove_n:\n",
        "        logger.warning(f\"Ú©Ø§Ø±Ø¨Ø± Ø¢ÛŒØªÙ… Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù {remove_n} Ù†Ø¯Ø§Ø±Ø¯.\")\n",
        "        continue\n",
        "\n",
        "    removed_items = list(user_all_items)[:remove_n]\n",
        "    train_mod = train.tolil(copy=True)\n",
        "    for it in removed_items:\n",
        "        train_mod[target_user, it] = 0.0\n",
        "\n",
        "    # ğŸ”¹ Ù†Ø³Ø®Ù‡ Ø³Ø±ÛŒØ¹\n",
        "    model_mod = LightFM(no_components=8, learning_rate=0.05, loss='warp')\n",
        "    model_mod.fit(train_mod, epochs=5, num_threads=4)\n",
        "\n",
        "    # ğŸ”¹ Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ùˆ Ú©Ù†Ø¯ (Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡)\n",
        "    # model_mod = LightFM(no_components=32, learning_rate=0.05, loss='warp')\n",
        "    # model_mod.fit(train_mod, epochs=20, num_threads=4)\n",
        "\n",
        "    top_items, scores = recommend_for_user(model_mod, train_mod, target_user, n=5)\n",
        "    correct_count = sum(1 for it in removed_items if it in top_items)\n",
        "    accuracy = correct_count / remove_n\n",
        "    logger.info(f\"ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù {remove_n} Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: {accuracy:.2f}\")\n",
        "\n",
        "    for it in top_items:\n",
        "        scenario_results.append({\n",
        "            'scenario': f'Ø­Ø°Ù {remove_n} Ø¢ÛŒØªÙ…',\n",
        "            'user_id': user_ids[target_user],\n",
        "            'product_id': item_ids[it],\n",
        "            'predicted_score': scores[it],\n",
        "            'is_target_item': item_ids[it] in [item_ids[x] for x in removed_items]\n",
        "        })\n",
        "\n",
        "pd.DataFrame(scenario_results).to_excel('scenario_results.xlsx', index=False)\n",
        "\n",
        "# ---------------------\n",
        "# 6) Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
        "# ---------------------\n",
        "def get_top_n_all_users(model, train_mat, user_map, item_map, n=5):\n",
        "    n_users, n_items = train_mat.shape\n",
        "    all_results = []\n",
        "    item_rev_map = {v: k for k, v in item_map.items()}\n",
        "    for uid, uidx in user_map.items():\n",
        "        scores = model.predict(np.repeat(uidx, n_items),\n",
        "                               np.arange(n_items),\n",
        "                               num_threads=4)\n",
        "        known_items = set(train_mat.tocsr()[uidx].indices)\n",
        "        candidates = [i for i in range(n_items) if i not in known_items]\n",
        "        top_idx = np.argsort(-scores[candidates])[:n]\n",
        "        for i in top_idx:\n",
        "            all_results.append({\n",
        "                'user_id': uid,\n",
        "                'product_id': item_rev_map[candidates[i]],\n",
        "                'predicted_score': scores[candidates[i]]\n",
        "            })\n",
        "    return all_results\n",
        "\n",
        "full_recs = get_top_n_all_users(model, train, user_map, item_map, n=5)\n",
        "pd.DataFrame(full_recs).to_excel('all_users_results.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90isKwfn-sfL",
        "outputId": "069fef60-2782-45a5-c92d-45d3f604b9f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:âœ… Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³: (4978, 11616)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-19 17:12:02,928 - INFO - âœ… Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³: (4978, 11616)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:âœ… Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-19 17:12:03,026 - INFO - âœ… Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù 1 Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-19 17:12:03,263 - INFO - ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù 1 Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù 2 Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-19 17:12:03,378 - INFO - ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù 2 Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù 3 Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-19 17:12:03,487 - INFO - ğŸ“Œ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø­Ø°Ù 3 Ø¢ÛŒØªÙ… â†’ Ø¯Ù‚Øª: 0.00\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}